{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hI8Pb0ykYtSg"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRf2-SLcZq1b",
        "outputId": "9f18efff-138a-4e88-a1e0-0e7044426377"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=d460d31757826ed7f409a1f2d1706324e63501c3ea76da1b525ef78dbf101213\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "id": "JQumZPXyajeV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "id": "jpGKqvh1a4P2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "6JO3FOAla-J5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n"
      ],
      "metadata": {
        "id": "An--YtxaZ2Vc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "SQbdsUiQZ9T6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n"
      ],
      "metadata": {
        "id": "hrZPaGY4bHYJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession and SparkContext\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"WordCount\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Initialize StreamingContext with a 10-second batch interval\n",
        "ssc = StreamingContext(sc, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOfAXX2CbNg2",
        "outputId": "fbcf8baa-5ee3-4e01-9827-eee6b7d7fb88"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.1-bin-hadoop3/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a directory for the stream files\n",
        "stream_dir = \"/content/input.txt\"\n",
        "if not os.path.exists(stream_dir):\n",
        "    os.mkdir(stream_dir)\n",
        "\n",
        "# Define the text file stream\n",
        "lines = ssc.textFileStream(stream_dir)\n"
      ],
      "metadata": {
        "id": "09C0-xEabPN7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njnst5GBcW17",
        "outputId": "880cbca4-6a0d-4c58-97f9-e713d413174e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.streaming.dstream.DStream at 0x7cc9259c5930>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split each line into words and count them\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the word counts to the console\n",
        "word_counts.pprint()\n"
      ],
      "metadata": {
        "id": "HSJgkOtrcM4W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvXrJ-XYcP3W",
        "outputId": "b337ac51-ca68-48be-bae9-edd06452d8f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.streaming.dstream.TransformedDStream at 0x7cc9240caad0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the streaming computation\n",
        "ssc.start()\n",
        "\n"
      ],
      "metadata": {
        "id": "_witOGdTcS7w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Create a series of files to simulate the stream\n",
        "for i in range(5):\n",
        "    with open(f\"/content/file{i}.txt\", \"w\") as f:\n",
        "        f.write(f\"This is a streaming example file {i}.\\nThis example shows streaming word count.\\n\")\n",
        "    time.sleep(10)  # Sleep for 10 seconds to simulate real-time data\n"
      ],
      "metadata": {
        "id": "5YfoKumCccEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b183f3-7a75-4808-e566-3325869b1072"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:49:50\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:50:00\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:50:10\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:50:20\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:50:30\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for the computation to terminate after 60 seconds\n",
        "#ssc.awaitTerminationOrTimeout(60)\n",
        "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
      ],
      "metadata": {
        "id": "ogwVNzmTc4jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120d63f5-e66c-491e-f301-ac73a07c9c27"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:50:40\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-09-20 17:50:50\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVHqjh83haGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_Y3GgReheDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}